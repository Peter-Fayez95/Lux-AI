{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9826416,"sourceType":"datasetVersion","datasetId":5993738}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from typing import Callable, Tuple\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-11-09T11:41:24.099933Z","iopub.execute_input":"2024-11-09T11:41:24.100288Z","iopub.status.idle":"2024-11-09T11:41:32.038496Z","shell.execute_reply.started":"2024-11-09T11:41:24.100240Z","shell.execute_reply":"2024-11-09T11:41:32.037489Z"},"trusted":true},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# Load CIFAR-10","metadata":{}},{"cell_type":"code","source":"transform = transforms.Compose(\n    [transforms.ToTensor(),\n     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n\nbatch_size = 128\n\ntrainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n                                        download=True, transform=transform)\n\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n                                          shuffle=True, num_workers=2)\n\ntestset = torchvision.datasets.CIFAR10(root='./data', train=False,\n                                       download=True, transform=transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n                                         shuffle=False, num_workers=2)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-09T11:42:35.937904Z","iopub.execute_input":"2024-11-09T11:42:35.938413Z","iopub.status.idle":"2024-11-09T11:42:41.155666Z","shell.execute_reply.started":"2024-11-09T11:42:35.938373Z","shell.execute_reply":"2024-11-09T11:42:41.154679Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 170498071/170498071 [00:01<00:00, 96406328.31it/s] \n","output_type":"stream"},{"name":"stdout","text":"Extracting ./data/cifar-10-python.tar.gz to ./data\nFiles already downloaded and verified\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"def get_default_device():\n    \"\"\"Pick GPU if available, else CPU\"\"\"\n    if torch.cuda.is_available():\n        return torch.device('cuda')\n    else:\n        return torch.device('cpu')\n    \ndef to_device(data, device):\n    \"\"\"Move tensor(s) to chosen device\"\"\"\n    if isinstance(data, (list,tuple)):\n        return [to_device(x, device) for x in data]\n    return data.to(device=device, non_blocking=True)\n\nclass DeviceDataLoader():\n    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n    def __init__(self, dl, device):\n        self.dl = dl\n        self.device = device\n        \n    def __iter__(self):\n        \"\"\"Yield a batch of data after moving it to device\"\"\"\n        for b in self.dl: \n            yield to_device(b, self.device)\n\n    def __len__(self):\n        \"\"\"Number of batches\"\"\"\n        return len(self.dl)","metadata":{"execution":{"iopub.status.busy":"2024-11-09T11:42:41.157311Z","iopub.execute_input":"2024-11-09T11:42:41.157919Z","iopub.status.idle":"2024-11-09T11:42:41.169761Z","shell.execute_reply.started":"2024-11-09T11:42:41.157883Z","shell.execute_reply":"2024-11-09T11:42:41.168357Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"code","source":"trainloader = DeviceDataLoader(trainloader, device)\ntestloader = DeviceDataLoader(testloader, device)","metadata":{"execution":{"iopub.status.busy":"2024-11-09T11:42:43.410666Z","iopub.execute_input":"2024-11-09T11:42:43.411514Z","iopub.status.idle":"2024-11-09T11:42:43.415558Z","shell.execute_reply.started":"2024-11-09T11:42:43.411473Z","shell.execute_reply":"2024-11-09T11:42:43.414683Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"code","source":"@torch.no_grad()\ndef evaluate(model, data):\n    correct = 0\n    total = 0\n    # since we're not training, we don't need to calculate the gradients for our outputs\n    with torch.no_grad():\n        for images, labels in data:\n            # calculate outputs by running images through the network\n            outputs = model(images.to(device))\n            # the class with the highest energy is what we choose as prediction\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels.to(device)).sum().item()\n\n    return 100 * correct / total","metadata":{"execution":{"iopub.status.busy":"2024-11-09T11:42:45.939764Z","iopub.execute_input":"2024-11-09T11:42:45.940174Z","iopub.status.idle":"2024-11-09T11:42:45.946950Z","shell.execute_reply.started":"2024-11-09T11:42:45.940136Z","shell.execute_reply":"2024-11-09T11:42:45.945975Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# Toad convmodel","metadata":{}},{"cell_type":"code","source":"class SELayer(nn.Module):\n    def __init__(self, n_channels: int, rescale_input: bool, reduction: int = 16):\n        super(SELayer, self).__init__()\n        self.rescale_input = rescale_input\n        self.fc = nn.Sequential(\n            nn.Linear(n_channels, n_channels // reduction, bias=False),\n            nn.ReLU(inplace=True),\n            nn.Linear(n_channels // reduction, n_channels, bias=False),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        b, c, _, _ = x.shape\n        # Average feature planes\n        if self.rescale_input:\n            y = torch.flatten(x, start_dim=-2, end_dim=-1).sum(dim=-1)\n        else:\n            y = torch.flatten(x, start_dim=-2, end_dim=-1).mean(dim=-1)\n        y = self.fc(y.view(b, c)).view(b, c, 1, 1)\n        return x * y.expand_as(x)\n\n\nclass ResidualBlock(nn.Module):\n    def __init__(\n            self,\n            in_channels: int,\n            out_channels: int,\n            height: int,\n            width: int,\n            kernel_size: int = 3,\n            normalize: bool = False,\n            activation: Callable = nn.ReLU,\n            squeeze_excitation: bool = True,\n            rescale_se_input: bool = True,\n            **conv2d_kwargs\n    ):\n        super(ResidualBlock, self).__init__()\n\n        # Calculate \"same\" padding\n        # https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n        # https://www.wolframalpha.com/input/?i=i%3D%28i%2B2x-k-%28k-1%29%28d-1%29%2Fs%29+%2B+1&assumption=%22i%22+-%3E+%22Variable%22\n        assert \"padding\" not in conv2d_kwargs.keys()\n        k = kernel_size\n        d = conv2d_kwargs.get(\"dilation\", 1)\n        s = conv2d_kwargs.get(\"stride\", 1)\n        padding = (k - 1) * (d + s - 1) / (2 * s)\n        assert padding == int(padding), f\"padding should be an integer, was {padding:.2f}\"\n        padding = int(padding)\n\n        self.conv1 = nn.Conv2d(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=(kernel_size, kernel_size),\n            padding=(padding, padding),\n            **conv2d_kwargs\n        )\n        # We use LayerNorm here since the size of the input \"images\" may vary based on the board size\n        self.norm1 = nn.LayerNorm([in_channels, height, width]) if normalize else nn.Identity()\n        self.act1 = activation()\n\n        self.conv2 = nn.Conv2d(\n            in_channels=out_channels,\n            out_channels=out_channels,\n            kernel_size=(kernel_size, kernel_size),\n            padding=(padding, padding),\n            **conv2d_kwargs\n        )\n        self.norm2 = nn.LayerNorm([in_channels, height, width]) if normalize else nn.Identity()\n        self.final_act = activation()\n\n        if in_channels != out_channels:\n            self.change_n_channels = nn.Conv2d(in_channels, out_channels, (1, 1))\n        else:\n            self.change_n_channels = nn.Identity()\n\n        if squeeze_excitation:\n            self.squeeze_excitation = SELayer(out_channels, rescale_se_input)\n        else:\n            self.squeeze_excitation = nn.Identity()\n\n    def forward(self, x):\n        identity = x\n        x = self.conv1(x)\n        x = self.act1(self.norm1(x))\n        x = self.conv2(x)\n        x = self.squeeze_excitation(self.norm2(x))\n        x = x + self.change_n_channels(identity)\n        return self.final_act(x) ","metadata":{"execution":{"iopub.status.busy":"2024-11-06T21:26:24.864082Z","iopub.execute_input":"2024-11-06T21:26:24.865264Z","iopub.status.idle":"2024-11-06T21:26:24.883523Z","shell.execute_reply.started":"2024-11-06T21:26:24.865210Z","shell.execute_reply":"2024-11-06T21:26:24.882567Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# The orginal makes overfit\n# toad = nn.Sequential(*[ResidualBlock(in_channels = 3, \n#                                     out_channels = 3, \n#                                     height = 32, \n#                                     width = 32,\n#                                     kernel_size = 3,\n#                                     normalize = False,\n#                                     activation = nn.LeakyReLU) for _ in range(5)],\n#                     nn.Flatten(),\n#                     nn.Linear(3072,1000),\n#                     nn.ReLU(),\n#                     nn.Linear(1000,100),\n#                     nn.ReLU(),\n#                     nn.Linear(100,10)).to(device=device)\n\ntoad = nn.Sequential(ResidualBlock(in_channels = 3, \n                                    out_channels = 32, \n                                    height = 32, \n                                    width = 32,\n                                    kernel_size = 5,\n                                    normalize = False,\n                                    activation = nn.LeakyReLU),\n                     ResidualBlock(in_channels = 32, \n                                    out_channels = 64, \n                                    height = 32, \n                                    width = 32,\n                                    kernel_size = 5,\n                                    normalize = False,\n                                    activation = nn.LeakyReLU),\n                    nn.MaxPool2d(2, 2),\n                    ResidualBlock(in_channels = 64, \n                                    out_channels = 128, \n                                    height = 16, \n                                    width = 16,\n                                    kernel_size = 5,\n                                    normalize = False,\n                                    activation = nn.LeakyReLU),\n                     ResidualBlock(in_channels = 128, \n                                    out_channels = 128, \n                                    height = 16, \n                                    width = 16,\n                                    kernel_size = 5,\n                                    normalize = False,\n                                    activation = nn.LeakyReLU),\n                    nn.MaxPool2d(2, 2), \n                    nn.Flatten(),\n                    nn.Linear(8*8*128,128),\n                    nn.ReLU(),\n                    nn.Linear(128,32),\n                    nn.ReLU(),\n                    nn.Linear(32,10)).to(device=device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(toad.parameters(), lr=1e-3, eps=0.0003)\n\nprint(count_parameters(toad))","metadata":{"execution":{"iopub.status.busy":"2024-11-03T02:38:53.190008Z","iopub.execute_input":"2024-11-03T02:38:53.190851Z","iopub.status.idle":"2024-11-03T02:38:53.231652Z","shell.execute_reply.started":"2024-11-03T02:38:53.190811Z","shell.execute_reply":"2024-11-03T02:38:53.230763Z"},"trusted":true},"outputs":[{"name":"stdout","text":"2684362\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"num_epochs = 21\n\nfor epoch in range(num_epochs):  # loop over the dataset multiple times\n    running_loss = 0.0\n    for inputs, labels in trainloader:\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward + backward + optimize\n        outputs = toad(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n    val = evaluate(toad, testloader)\n    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Val_acc: {val:.2f}')\n\nprint('Finished Training')","metadata":{"execution":{"iopub.status.busy":"2024-11-03T02:38:57.849846Z","iopub.execute_input":"2024-11-03T02:38:57.850265Z","iopub.status.idle":"2024-11-03T02:44:48.934997Z","shell.execute_reply.started":"2024-11-03T02:38:57.850227Z","shell.execute_reply":"2024-11-03T02:44:48.933751Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Epoch [1/21], Loss: 1.5276, Val_acc: 48.08\nEpoch [2/21], Loss: 1.1505, Val_acc: 54.64\nEpoch [3/21], Loss: 1.0087, Val_acc: 65.65\nEpoch [4/21], Loss: 0.9043, Val_acc: 70.13\nEpoch [5/21], Loss: 0.6795, Val_acc: 73.12\nEpoch [6/21], Loss: 0.4675, Val_acc: 73.91\nEpoch [7/21], Loss: 0.5149, Val_acc: 75.03\nEpoch [8/21], Loss: 0.3057, Val_acc: 73.92\nEpoch [9/21], Loss: 0.2839, Val_acc: 75.25\nEpoch [10/21], Loss: 0.1420, Val_acc: 73.83\nEpoch [11/21], Loss: 0.1388, Val_acc: 73.42\nEpoch [12/21], Loss: 0.1576, Val_acc: 73.61\nEpoch [13/21], Loss: 0.1924, Val_acc: 74.39\nEpoch [14/21], Loss: 0.0829, Val_acc: 73.42\nEpoch [15/21], Loss: 0.1561, Val_acc: 74.42\nEpoch [16/21], Loss: 0.0849, Val_acc: 74.62\nEpoch [17/21], Loss: 0.1289, Val_acc: 73.66\nEpoch [18/21], Loss: 0.0394, Val_acc: 73.89\nEpoch [19/21], Loss: 0.0683, Val_acc: 74.26\nEpoch [20/21], Loss: 0.0133, Val_acc: 74.65\nEpoch [21/21], Loss: 0.0843, Val_acc: 73.88\nFinished Training\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"correct = 0\ntotal = 0\n# since we're not training, we don't need to calculate the gradients for our outputs\nwith torch.no_grad():\n    for data in trainloader:\n        images, labels = data\n        # calculate outputs by running images through the network\n        outputs = toad(images.to(device))\n        # the class with the highest energy is what we choose as prediction\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels.to(device)).sum().item()\n\nprint(f'Accuracy of the network on the 10000 test images: {100 * correct / total:.2f} %')","metadata":{"execution":{"iopub.status.busy":"2024-11-03T02:44:48.937578Z","iopub.execute_input":"2024-11-03T02:44:48.938035Z","iopub.status.idle":"2024-11-03T02:44:56.702511Z","shell.execute_reply.started":"2024-11-03T02:44:48.937983Z","shell.execute_reply":"2024-11-03T02:44:56.701352Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Accuracy of the network on the 10000 test images: 98.30 %\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"# Vim Original implementation","metadata":{}},{"cell_type":"code","source":"import os\nimport shutil\nimport sys\n\ndestination = shutil.copytree('/kaggle/input/vim-implementation/Vim-main/Vim-main', '/kaggle/working/vim')\n#destination = shutil.copytree('/kaggle/input/vim-implementation/vim_compiled/vim', '/kaggle/working/vim')","metadata":{"execution":{"iopub.status.busy":"2024-11-08T12:57:42.800410Z","iopub.execute_input":"2024-11-08T12:57:42.801183Z","iopub.status.idle":"2024-11-08T12:57:48.831452Z","shell.execute_reply.started":"2024-11-08T12:57:42.801137Z","shell.execute_reply":"2024-11-08T12:57:48.830393Z"},"trusted":true},"outputs":[],"execution_count":12},{"cell_type":"code","source":"!pip install -e vim/mamba-1p1p1","metadata":{"execution":{"iopub.status.busy":"2024-11-08T12:57:48.832951Z","iopub.execute_input":"2024-11-08T12:57:48.833316Z","iopub.status.idle":"2024-11-08T13:18:10.121649Z","shell.execute_reply.started":"2024-11-08T12:57:48.833282Z","shell.execute_reply":"2024-11-08T13:18:10.120600Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Obtaining file:///kaggle/working/vim/mamba-1p1p1\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from mamba_ssm==1.1.1) (2.4.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from mamba_ssm==1.1.1) (21.3)\nRequirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from mamba_ssm==1.1.1) (1.11.1.1)\nRequirement already satisfied: einops in /opt/conda/lib/python3.10/site-packages (from mamba_ssm==1.1.1) (0.8.0)\nCollecting triton (from mamba_ssm==1.1.1)\n  Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from mamba_ssm==1.1.1) (4.45.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->mamba_ssm==1.1.1) (3.1.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->mamba_ssm==1.1.1) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->mamba_ssm==1.1.1) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->mamba_ssm==1.1.1) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->mamba_ssm==1.1.1) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->mamba_ssm==1.1.1) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->mamba_ssm==1.1.1) (2024.6.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers->mamba_ssm==1.1.1) (0.25.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers->mamba_ssm==1.1.1) (1.26.4)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers->mamba_ssm==1.1.1) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->mamba_ssm==1.1.1) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers->mamba_ssm==1.1.1) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers->mamba_ssm==1.1.1) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers->mamba_ssm==1.1.1) (0.20.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers->mamba_ssm==1.1.1) (4.66.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->mamba_ssm==1.1.1) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->mamba_ssm==1.1.1) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->mamba_ssm==1.1.1) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->mamba_ssm==1.1.1) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->mamba_ssm==1.1.1) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->mamba_ssm==1.1.1) (1.3.0)\nDownloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: triton, mamba_ssm\n  Running setup.py develop for mamba_ssm\nSuccessfully installed mamba_ssm-1.1.1 triton-3.1.0\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"!pip install -e vim/causal-conv1d-1.1.0","metadata":{"execution":{"iopub.status.busy":"2024-11-08T13:18:10.123562Z","iopub.execute_input":"2024-11-08T13:18:10.123892Z","iopub.status.idle":"2024-11-08T13:20:12.890972Z","shell.execute_reply.started":"2024-11-08T13:18:10.123852Z","shell.execute_reply":"2024-11-08T13:20:12.889565Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Obtaining file:///kaggle/working/vim/causal-conv1d-1.1.0\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from causal_conv1d==1.1.0) (2.4.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from causal_conv1d==1.1.0) (21.3)\nCollecting buildtools (from causal_conv1d==1.1.0)\n  Downloading buildtools-1.0.6.tar.gz (446 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m446.5/446.5 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from causal_conv1d==1.1.0) (1.11.1.1)\nRequirement already satisfied: sqlalchemy in /opt/conda/lib/python3.10/site-packages (from buildtools->causal_conv1d==1.1.0) (2.0.30)\nCollecting argparse (from buildtools->causal_conv1d==1.1.0)\n  Downloading argparse-1.4.0-py2.py3-none-any.whl.metadata (2.8 kB)\nCollecting twisted (from buildtools->causal_conv1d==1.1.0)\n  Downloading twisted-24.10.0-py3-none-any.whl.metadata (20 kB)\nCollecting simplejson (from buildtools->causal_conv1d==1.1.0)\n  Downloading simplejson-3.19.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.2 kB)\nCollecting furl (from buildtools->causal_conv1d==1.1.0)\n  Downloading furl-2.1.3-py2.py3-none-any.whl.metadata (1.2 kB)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from buildtools->causal_conv1d==1.1.0) (2.32.3)\nRequirement already satisfied: docopt in /opt/conda/lib/python3.10/site-packages (from buildtools->causal_conv1d==1.1.0) (0.6.2)\nRequirement already satisfied: python-dateutil in /opt/conda/lib/python3.10/site-packages (from buildtools->causal_conv1d==1.1.0) (2.9.0.post0)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from buildtools->causal_conv1d==1.1.0) (3.1.4)\nCollecting redo (from buildtools->causal_conv1d==1.1.0)\n  Downloading redo-3.0.0-py2.py3-none-any.whl.metadata (8.0 kB)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->causal_conv1d==1.1.0) (3.1.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->causal_conv1d==1.1.0) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->causal_conv1d==1.1.0) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->causal_conv1d==1.1.0) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->causal_conv1d==1.1.0) (3.3)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->causal_conv1d==1.1.0) (2024.6.1)\nRequirement already satisfied: six>=1.8.0 in /opt/conda/lib/python3.10/site-packages (from furl->buildtools->causal_conv1d==1.1.0) (1.16.0)\nCollecting orderedmultidict>=1.0.1 (from furl->buildtools->causal_conv1d==1.1.0)\n  Downloading orderedmultidict-1.0.1-py2.py3-none-any.whl.metadata (1.3 kB)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->buildtools->causal_conv1d==1.1.0) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->buildtools->causal_conv1d==1.1.0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->buildtools->causal_conv1d==1.1.0) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->buildtools->causal_conv1d==1.1.0) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->buildtools->causal_conv1d==1.1.0) (2024.8.30)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from sqlalchemy->buildtools->causal_conv1d==1.1.0) (3.0.3)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->causal_conv1d==1.1.0) (1.3.0)\nRequirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.10/site-packages (from twisted->buildtools->causal_conv1d==1.1.0) (23.2.0)\nCollecting automat>=24.8.0 (from twisted->buildtools->causal_conv1d==1.1.0)\n  Downloading Automat-24.8.1-py3-none-any.whl.metadata (8.4 kB)\nCollecting constantly>=15.1 (from twisted->buildtools->causal_conv1d==1.1.0)\n  Downloading constantly-23.10.4-py3-none-any.whl.metadata (1.8 kB)\nCollecting hyperlink>=17.1.1 (from twisted->buildtools->causal_conv1d==1.1.0)\n  Downloading hyperlink-21.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\nCollecting incremental>=24.7.0 (from twisted->buildtools->causal_conv1d==1.1.0)\n  Downloading incremental-24.7.2-py3-none-any.whl.metadata (8.1 kB)\nCollecting zope-interface>=5 (from twisted->buildtools->causal_conv1d==1.1.0)\n  Downloading zope.interface-7.1.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (44 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: setuptools>=61.0 in /opt/conda/lib/python3.10/site-packages (from incremental>=24.7.0->twisted->buildtools->causal_conv1d==1.1.0) (70.0.0)\nRequirement already satisfied: tomli in /opt/conda/lib/python3.10/site-packages (from incremental>=24.7.0->twisted->buildtools->causal_conv1d==1.1.0) (2.0.1)\nDownloading argparse-1.4.0-py2.py3-none-any.whl (23 kB)\nDownloading furl-2.1.3-py2.py3-none-any.whl (20 kB)\nDownloading redo-3.0.0-py2.py3-none-any.whl (14 kB)\nDownloading simplejson-3.19.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (137 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.9/137.9 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading twisted-24.10.0-py3-none-any.whl (3.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m56.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading Automat-24.8.1-py3-none-any.whl (42 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading constantly-23.10.4-py3-none-any.whl (13 kB)\nDownloading hyperlink-21.0.0-py2.py3-none-any.whl (74 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.6/74.6 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading incremental-24.7.2-py3-none-any.whl (20 kB)\nDownloading orderedmultidict-1.0.1-py2.py3-none-any.whl (11 kB)\nDownloading zope.interface-7.1.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (254 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.2/254.2 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: buildtools\n  Building wheel for buildtools (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for buildtools: filename=buildtools-1.0.6-py3-none-any.whl size=512341 sha256=951d3412c70cfecab6c9ae83aa16f73911f644be535dc5fdc4854c7a3e83ab11\n  Stored in directory: /root/.cache/pip/wheels/90/e9/2a/625d99dffa430d0b4293d3d386f63e0eb8edeeb54f3f29d208\nSuccessfully built buildtools\nInstalling collected packages: redo, argparse, zope-interface, simplejson, orderedmultidict, incremental, hyperlink, constantly, automat, twisted, furl, buildtools, causal_conv1d\n  Running setup.py develop for causal_conv1d\nSuccessfully installed argparse-1.4.0 automat-24.8.1 buildtools-1.0.6 causal_conv1d-1.1.0 constantly-23.10.4 furl-2.1.3 hyperlink-21.0.0 incremental-24.7.2 orderedmultidict-1.0.1 redo-3.0.0 simplejson-3.19.3 twisted-24.10.0 zope-interface-7.1.1\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# Add path to environment\n#!export PATH=\"$PATH:/kaggle/working/vim/mamba-1p1p1\"\nsys.path.insert(0, \"/kaggle/working/vim/mamba-1p1p1\")\nsys.path.insert(0, \"/kaggle/working/vim/causal-conv1d-1.1.0\")","metadata":{"execution":{"iopub.status.busy":"2024-11-08T13:20:12.892709Z","iopub.execute_input":"2024-11-08T13:20:12.893080Z","iopub.status.idle":"2024-11-08T13:20:12.902624Z","shell.execute_reply.started":"2024-11-08T13:20:12.893037Z","shell.execute_reply":"2024-11-08T13:20:12.898198Z"},"trusted":true},"outputs":[],"execution_count":15},{"cell_type":"code","source":"from vim.vim.models_mamba import VisionMamba\n\n#vimamba = VisionMamba(img_size=32, patch_size=4, stride=2, depth = 5, emed_dim = 192, d_state=16, channels = 3, num_classes=10).to(device=device)\nvimamba = VisionMamba(img_size = 32, patch_size = 4, stride = 2, depth = 5, emed_dim = 192, d_state=16, channels = 3, num_classes=10, if_bidirectional = False, if_bimamba = False,  drop_path_rate=0.0,)\n#vimamba = VisionMamba(img_size = 32, patch_size=16, stride = 8, embed_dim=192, depth=5, d_state=16, rms_norm=True, residual_in_fp32=True, fused_add_norm=True, final_pool_type='mean', if_abs_pos_embed=True, if_rope=True, if_rope_residual=True, bimamba_type=\"v2\", if_cls_token=True)\nvimamba = nn.DataParallel(vimamba)\nvimamba.to('cuda')\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(vimamba.parameters(), lr=1e-3, eps=0.0003)\n\nprint(count_parameters(vimamba))","metadata":{"execution":{"iopub.status.busy":"2024-11-08T13:20:12.905644Z","iopub.execute_input":"2024-11-08T13:20:12.906045Z","iopub.status.idle":"2024-11-08T13:20:14.009607Z","shell.execute_reply.started":"2024-11-08T13:20:12.905985Z","shell.execute_reply":"2024-11-08T13:20:14.008598Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/kaggle/working/vim/mamba-1p1p1/mamba_ssm/ops/selective_scan_interface.py:159: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  def forward(ctx, xz, conv1d_weight, conv1d_bias, x_proj_weight, delta_proj_weight,\n/kaggle/working/vim/mamba-1p1p1/mamba_ssm/ops/selective_scan_interface.py:228: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n  def backward(ctx, dout):\n/kaggle/working/vim/mamba-1p1p1/mamba_ssm/ops/selective_scan_interface.py:296: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  def forward(ctx, xz, conv1d_weight, conv1d_bias, x_proj_weight, delta_proj_weight,\n/kaggle/working/vim/mamba-1p1p1/mamba_ssm/ops/selective_scan_interface.py:369: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n  def backward(ctx, dout):\n/kaggle/working/vim/mamba-1p1p1/mamba_ssm/ops/selective_scan_interface.py:441: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  def forward(ctx, xz, conv1d_weight, conv1d_bias, x_proj_weight, delta_proj_weight,\n/kaggle/working/vim/mamba-1p1p1/mamba_ssm/ops/selective_scan_interface.py:521: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n  def backward(ctx, dout):\n/kaggle/working/vim/mamba-1p1p1/mamba_ssm/ops/triton/layernorm.py:508: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  def forward(\n/kaggle/working/vim/mamba-1p1p1/mamba_ssm/ops/triton/layernorm.py:567: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n  def backward(ctx, dout, *args):\n","output_type":"stream"},{"name":"stdout","text":"1465354\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"num_epochs = 21\nfor epoch in range(num_epochs):  # loop over the dataset multiple times\n    for inputs, labels in trainloader:\n        # zero the parameter gradients\n        optimizer.zero_grad()\n        # print(inputs.dtype)\n        # forward + backward + optimize\n        outputs = vimamba(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n    val = evaluate(vimamba, testloader)\n    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Val_acc: {val:.2f}')\n\nprint('Finished Training')","metadata":{"execution":{"iopub.status.busy":"2024-11-08T13:20:14.010903Z","iopub.execute_input":"2024-11-08T13:20:14.011486Z","iopub.status.idle":"2024-11-08T13:44:00.984653Z","shell.execute_reply.started":"2024-11-08T13:20:14.011439Z","shell.execute_reply":"2024-11-08T13:44:00.983427Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/21], Loss: 1.4626, Val_acc: 48.47\nEpoch [2/21], Loss: 1.1322, Val_acc: 58.30\nEpoch [3/21], Loss: 1.0304, Val_acc: 64.59\nEpoch [4/21], Loss: 0.7788, Val_acc: 67.81\nEpoch [5/21], Loss: 0.7452, Val_acc: 70.26\nEpoch [6/21], Loss: 0.4161, Val_acc: 70.19\nEpoch [7/21], Loss: 0.4232, Val_acc: 72.43\nEpoch [8/21], Loss: 0.4424, Val_acc: 71.55\nEpoch [9/21], Loss: 0.3366, Val_acc: 72.12\nEpoch [10/21], Loss: 0.3010, Val_acc: 72.03\nEpoch [11/21], Loss: 0.3560, Val_acc: 72.38\nEpoch [12/21], Loss: 0.2432, Val_acc: 72.30\nEpoch [13/21], Loss: 0.2122, Val_acc: 72.39\nEpoch [14/21], Loss: 0.2409, Val_acc: 72.12\nEpoch [15/21], Loss: 0.2068, Val_acc: 72.10\nEpoch [16/21], Loss: 0.0801, Val_acc: 72.58\nEpoch [17/21], Loss: 0.1370, Val_acc: 72.99\nEpoch [18/21], Loss: 0.0842, Val_acc: 71.86\nEpoch [19/21], Loss: 0.1323, Val_acc: 72.91\nEpoch [20/21], Loss: 0.0703, Val_acc: 72.67\nEpoch [21/21], Loss: 0.1188, Val_acc: 73.09\nFinished Training\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"correct = 0\ntotal = 0\n# since we're not training, we don't need to calculate the gradients for our outputs\nwith torch.no_grad():\n    for data in trainloader:\n        images, labels = data\n        # calculate outputs by running images through the network\n        outputs = vimamba(images)\n        # the class with the highest energy is what we choose as prediction\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels.to(device)).sum().item()\n\nprint(f'Accuracy of the network on the 10000 test images: {100 * correct / total:.2f} %')","metadata":{"execution":{"iopub.status.busy":"2024-11-08T13:44:11.030103Z","iopub.execute_input":"2024-11-08T13:44:11.030528Z","iopub.status.idle":"2024-11-08T13:44:31.763151Z","shell.execute_reply.started":"2024-11-08T13:44:11.030489Z","shell.execute_reply":"2024-11-08T13:44:31.762001Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Accuracy of the network on the 10000 test images: 97.99 %\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"# SiMBA","metadata":{}},{"cell_type":"code","source":"!pip install einops\n!pip install mamba-ssm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-09T11:43:16.810111Z","iopub.execute_input":"2024-11-09T11:43:16.810978Z","iopub.status.idle":"2024-11-09T11:44:04.854903Z","shell.execute_reply.started":"2024-11-09T11:43:16.810937Z","shell.execute_reply":"2024-11-09T11:44:04.853863Z"}},"outputs":[{"name":"stdout","text":"Collecting einops\n  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\nDownloading einops-0.8.0-py3-none-any.whl (43 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: einops\nSuccessfully installed einops-0.8.0\nCollecting mamba-ssm\n  Downloading mamba_ssm-2.2.2.tar.gz (85 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from mamba-ssm) (2.4.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from mamba-ssm) (21.3)\nRequirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from mamba-ssm) (1.11.1.1)\nRequirement already satisfied: einops in /opt/conda/lib/python3.10/site-packages (from mamba-ssm) (0.8.0)\nCollecting triton (from mamba-ssm)\n  Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from mamba-ssm) (4.45.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->mamba-ssm) (3.1.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->mamba-ssm) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->mamba-ssm) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->mamba-ssm) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->mamba-ssm) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->mamba-ssm) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->mamba-ssm) (2024.6.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers->mamba-ssm) (0.25.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers->mamba-ssm) (1.26.4)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers->mamba-ssm) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->mamba-ssm) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers->mamba-ssm) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers->mamba-ssm) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers->mamba-ssm) (0.20.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers->mamba-ssm) (4.66.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->mamba-ssm) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->mamba-ssm) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->mamba-ssm) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->mamba-ssm) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->mamba-ssm) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->mamba-ssm) (1.3.0)\nDownloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: mamba-ssm\n  Building wheel for mamba-ssm (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for mamba-ssm: filename=mamba_ssm-2.2.2-cp310-cp310-linux_x86_64.whl size=323998290 sha256=a658a5438dbe9fb3a53799d7d9f4714ca09225769c24ec04a403728ac7d1c69e\n  Stored in directory: /root/.cache/pip/wheels/57/7c/90/9f963468ecc3791e36e388f9e7b4a4e1e3f90fbb340055aa4d\nSuccessfully built mamba-ssm\nInstalling collected packages: triton, mamba-ssm\nSuccessfully installed mamba-ssm-2.2.2 triton-3.1.0\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom functools import partial\nimport torch.fft\n\nfrom timm.models.layers import DropPath, to_2tuple, trunc_normal_\nfrom timm.models.registry import register_model\nfrom timm.models.vision_transformer import _cfg\nfrom mamba_ssm import Mamba\nimport math\nimport numpy as np\nfrom mamba_ssm import Mamba\nfrom einops import rearrange, repeat, einsum","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-08T21:54:10.119819Z","iopub.execute_input":"2024-11-08T21:54:10.120651Z","iopub.status.idle":"2024-11-08T21:54:13.015075Z","shell.execute_reply.started":"2024-11-08T21:54:10.120599Z","shell.execute_reply":"2024-11-08T21:54:13.013962Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/mamba_ssm/ops/selective_scan_interface.py:164: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  def forward(ctx, xz, conv1d_weight, conv1d_bias, x_proj_weight, delta_proj_weight,\n/opt/conda/lib/python3.10/site-packages/mamba_ssm/ops/selective_scan_interface.py:240: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n  def backward(ctx, dout):\n/opt/conda/lib/python3.10/site-packages/mamba_ssm/ops/triton/layer_norm.py:986: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  def forward(\n/opt/conda/lib/python3.10/site-packages/mamba_ssm/ops/triton/layer_norm.py:1045: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n  def backward(ctx, dout, *args):\n/opt/conda/lib/python3.10/site-packages/mamba_ssm/distributed/tensor_parallel.py:26: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  def forward(ctx, x, weight, bias, process_group=None, sequence_parallel=True):\n/opt/conda/lib/python3.10/site-packages/mamba_ssm/distributed/tensor_parallel.py:62: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n  def backward(ctx, grad_output):\n/opt/conda/lib/python3.10/site-packages/mamba_ssm/ops/triton/ssd_combined.py:758: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  def forward(ctx, zxbcdt, conv1d_weight, conv1d_bias, dt_bias, A, D, chunk_size, initial_states=None, seq_idx=None, dt_limit=(0.0, float(\"inf\")), return_final_states=False, activation=\"silu\",\n/opt/conda/lib/python3.10/site-packages/mamba_ssm/ops/triton/ssd_combined.py:836: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n  def backward(ctx, dout, *args):\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"class EinFFT(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.hidden_size = dim #768\n        self.num_blocks = 4 \n        self.block_size = self.hidden_size // self.num_blocks \n        assert self.hidden_size % self.num_blocks == 0\n        self.sparsity_threshold = 0.01\n        self.scale = 0.02\n\n        self.complex_weight_1 = nn.Parameter(torch.randn(2, self.num_blocks, self.block_size, self.block_size, dtype=torch.float32) * self.scale)\n        self.complex_weight_2 = nn.Parameter(torch.randn(2, self.num_blocks, self.block_size, self.block_size, dtype=torch.float32) * self.scale)\n        self.complex_bias_1 = nn.Parameter(torch.randn(2, self.num_blocks, self.block_size,  dtype=torch.float32) * self.scale)\n        self.complex_bias_2 = nn.Parameter(torch.randn(2, self.num_blocks, self.block_size,  dtype=torch.float32) * self.scale)\n\n    def multiply(self, input, weights):\n        return torch.einsum('...bd,bdk->...bk', input, weights)\n\n    def forward(self, x, H, W):\n        B, N, C = x.shape\n        x = x.view(B, N, self.num_blocks, self.block_size )\n\n        x = torch.fft.fft2(x, dim=(1,2), norm='ortho') # FFT on N dimension\n\n        x_real_1 = F.relu(self.multiply(x.real, self.complex_weight_1[0]) - self.multiply(x.imag, self.complex_weight_1[1]) + self.complex_bias_1[0])\n        x_imag_1 = F.relu(self.multiply(x.real, self.complex_weight_1[1]) + self.multiply(x.imag, self.complex_weight_1[0]) + self.complex_bias_1[1])\n        x_real_2 = self.multiply(x_real_1, self.complex_weight_2[0]) - self.multiply(x_imag_1, self.complex_weight_2[1]) + self.complex_bias_2[0]\n        x_imag_2 = self.multiply(x_real_1, self.complex_weight_2[1]) + self.multiply(x_imag_1, self.complex_weight_2[0]) + self.complex_bias_2[1]\n\n        x = torch.stack([x_real_2, x_imag_2], dim=-1).float()\n        x = F.softshrink(x, lambd=self.sparsity_threshold) if self.sparsity_threshold else x\n        x = torch.view_as_complex(x)\n\n        x = torch.fft.ifft2(x, dim=(1,2), norm=\"ortho\")\n        \n        # RuntimeError: \"fused_dropout\" not implemented for 'ComplexFloat'\n        x = x.to(torch.float32)\n        x = x.reshape(B, N, C)\n        return x\n\n# For Fast Implementation use MambaLayer,# This implementation is slow, only for checking GFLOPS and other paramater,\n# For more details please refer to https://github.com/johnma2006/mamba-minimal/blob/master/model.py     \nclass MambaBlock(nn.Module):\n    def __init__(self, d_model, d_state = 64, expand = 2, d_conv = 4, conv_bias = True,  bias = False ):\n        \"\"\"A single Mamba block, as described in Figure 3 in Section 3.4 in the Mamba paper [1].\"\"\"\n        super().__init__()\n        self.d_model = d_model # Model dimension d_model\n        self.d_state=d_state # SSM state expansion factor\n        self.d_conv=d_conv  # Local convolution width\n        self.expand=expand  # Block expansion factor\n        self.conv_bias=conv_bias\n        self.bias=bias\n        self.d_inner = int(self.expand * self.d_model)\n        self.dt_rank = math.ceil(self.d_model / 16)\n        \n\n        self.in_proj = nn.Linear(self.d_model, self.d_inner * 2, bias=self.bias)\n\n        self.conv1d = nn.Conv1d(\n            in_channels=self.d_inner,\n            out_channels=self.d_inner,\n            bias=self.conv_bias,\n            kernel_size=self.d_conv,\n            groups=self.d_inner,\n            padding=self.d_conv - 1,\n        )\n\n        # x_proj takes in `x` and outputs the input-specific Δ, B, C\n        self.x_proj = nn.Linear(self.d_inner, self.dt_rank + self.d_state * 2, bias=False)\n        \n        # dt_proj projects Δ from dt_rank to d_in\n        self.dt_proj = nn.Linear(self.dt_rank, self.d_inner, bias=True)\n\n        A = repeat(torch.arange(1, self.d_state + 1), 'n -> d n', d=self.d_inner)\n        self.A_log = nn.Parameter(torch.log(A))\n        self.D = nn.Parameter(torch.ones(self.d_inner))\n        self.out_proj = nn.Linear(self.d_inner, self.d_model, bias=self.bias)\n        \n\n    def forward(self, x):\n        \"\"\"Mamba block forward. This looks the same as Figure 3 in Section 3.4 in the Mamba paper [1].\n    \n        Args:\n            x: shape (b, l, d)    (See Glossary at top for definitions of b, l, d_in, n...)\n    \n        Returns:\n            output: shape (b, l, d)\n        \n        Official Implementation:\n            class Mamba, https://github.com/state-spaces/mamba/blob/main/mamba_ssm/modules/mamba_simple.py#L119\n            mamba_inner_ref(), https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/selective_scan_interface.py#L311\n            \n        \"\"\"\n        (b, l, d) = x.shape\n        \n        x_and_res = self.in_proj(x)  # shape (b, l, 2 * d_in)\n        (x, res) = x_and_res.split(split_size=[self.d_inner, self.d_inner], dim=-1)\n\n        x = rearrange(x, 'b l d_in -> b d_in l')\n        x = self.conv1d(x)[:, :, :l]\n        x = rearrange(x, 'b d_in l -> b l d_in')\n        \n        x = F.silu(x)\n\n        y = self.ssm(x)\n        \n        y = y * F.silu(res)\n        \n        output = self.out_proj(y)\n\n        return output\n\n    \n    def ssm(self, x):\n        \"\"\"Runs the SSM. See:\n            - Algorithm 2 in Section 3.2 in the Mamba paper [1]\n            - run_SSM(A, B, C, u) in The Annotated S4 [2]\n\n        Args:\n            x: shape (b, l, d_in)    (See Glossary at top for definitions of b, l, d_in, n...)\n    \n        Returns:\n            output: shape (b, l, d_in)\n\n        Official Implementation:\n            mamba_inner_ref(), https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/selective_scan_interface.py#L311\n            \n        \"\"\"\n        (d_in, n) = self.A_log.shape\n\n        # Compute ∆ A B C D, the state space parameters.\n        #     A, D are input independent (see Mamba paper [1] Section 3.5.2 \"Interpretation of A\" for why A isn't selective)\n        #     ∆, B, C are input-dependent (this is a key difference between Mamba and the linear time invariant S4,\n        #                                  and is why Mamba is called **selective** state spaces)\n        \n        A = -torch.exp(self.A_log.float())  # shape (d_in, n)\n        D = self.D.float()\n\n        x_dbl = self.x_proj(x)  # (b, l, dt_rank + 2*n)\n        \n        (delta, B, C) = x_dbl.split(split_size=[self.dt_rank, n, n], dim=-1)  # delta: (b, l, dt_rank). B, C: (b, l, n)\n        delta = F.softplus(self.dt_proj(delta))  # (b, l, d_in)\n        \n        y = self.selective_scan(x, delta, A, B, C, D)  # This is similar to run_SSM(A, B, C, u) in The Annotated S4 [2]\n        \n        return y\n\n    \n    def selective_scan(self, u, delta, A, B, C, D):\n        \"\"\"Does selective scan algorithm. See:\n            - Section 2 State Space Models in the Mamba paper [1]\n            - Algorithm 2 in Section 3.2 in the Mamba paper [1]\n            - run_SSM(A, B, C, u) in The Annotated S4 [2]\n\n        This is the classic discrete state space formula:\n            x(t + 1) = Ax(t) + Bu(t)\n            y(t)     = Cx(t) + Du(t)\n        except B and C (and the step size delta, which is used for discretization) are dependent on the input x(t).\n    \n        Args:\n            u: shape (b, l, d_in)    (See Glossary at top for definitions of b, l, d_in, n...)\n            delta: shape (b, l, d_in)\n            A: shape (d_in, n)\n            B: shape (b, l, n)\n            C: shape (b, l, n)\n            D: shape (d_in,)\n    \n        Returns:\n            output: shape (b, l, d_in)\n    \n        Official Implementation:\n            selective_scan_ref(), https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/selective_scan_interface.py#L86\n            Note: I refactored some parts out of `selective_scan_ref` out, so the functionality doesn't match exactly.\n            \n        \"\"\"\n        (b, l, d_in) = u.shape\n        n = A.shape[1]\n        \n        # Discretize continuous parameters (A, B)\n        # - A is discretized using zero-order hold (ZOH) discretization (see Section 2 Equation 4 in the Mamba paper [1])\n        # - B is discretized using a simplified Euler discretization instead of ZOH. From a discussion with authors:\n        #   \"A is the more important term and the performance doesn't change much with the simplification on B\"\n        deltaA = torch.exp(einsum(delta, A, 'b l d_in, d_in n -> b l d_in n'))\n        deltaB_u = einsum(delta, B, u, 'b l d_in, b l n, b l d_in -> b l d_in n')\n        \n        # Perform selective scan (see scan_SSM() in The Annotated S4 [2])\n        # Note that the below is sequential, while the official implementation does a much faster parallel scan that\n        # is additionally hardware-aware (like FlashAttention).\n        x = torch.zeros((b, d_in, n), device=deltaA.device)\n        ys = []    \n        for i in range(l):\n            x = deltaA[:, i] * x + deltaB_u[:, i]\n            y = einsum(x, C[:, i, :], 'b d_in n, b n -> b d_in')\n            ys.append(y)\n        y = torch.stack(ys, dim=1)  # shape (b, l, d_in)\n        \n        y = y + u * D\n    \n        return y\n\n\nclass MambaLayer(nn.Module):\n    def __init__(self, dim, d_state=64, d_conv=4, expand=2):\n        super().__init__()\n        self.dim = dim\n        self.norm = nn.LayerNorm(dim)\n        self.mamba = Mamba(\n            d_model=dim,  # Model dimension d_model\n            d_state=d_state,  # SSM state expansion factor\n            d_conv=d_conv,  # Local convolution width\n            expand=expand  # Block expansion factor\n        )\n    def forward(self, x):\n        # print('x',x.shape)\n        B, L, C = x.shape\n        x_norm = self.norm(x)\n        x_mamba = self.mamba(x_norm)    \n        return x_mamba\n\ndef rand_bbox(size, lam, scale=1):\n    W = size[1] // scale\n    H = size[2] // scale\n    cut_rat = np.sqrt(1. - lam)\n    cut_w = np.int_(W * cut_rat)\n    cut_h = np.int_(H * cut_rat)\n\n    # uniform\n    cx = np.random.randint(W)\n    cy = np.random.randint(H)\n\n    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n    bby1 = np.clip(cy - cut_h // 2, 0, H)\n    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n    bby2 = np.clip(cy + cut_h // 2, 0, H)\n\n    return bbx1, bby1, bbx2, bby2\n\nclass PVT2FFN(nn.Module):\n    def __init__(self, in_features, hidden_features):\n        super().__init__()\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.dwconv = DWConv(hidden_features)\n        self.act = nn.GELU()\n        self.fc2 = nn.Linear(hidden_features, in_features)\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n        elif isinstance(m, nn.Conv2d):\n            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n            fan_out //= m.groups\n            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n            if m.bias is not None:\n                m.bias.data.zero_()\n\n    def forward(self, x, H, W):\n        x = self.fc1(x)\n        x = self.dwconv(x, H, W)\n        x = self.act(x)\n        x = self.fc2(x)\n        return x\n\nclass FFN(nn.Module):\n    def __init__(self, in_features, hidden_features):\n        super().__init__()\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = nn.GELU()\n        self.fc2 = nn.Linear(hidden_features, in_features)\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n        elif isinstance(m, nn.Conv2d):\n            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n            fan_out //= m.groups\n            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n            if m.bias is not None:\n                m.bias.data.zero_()\n\n    def forward(self, x, H, W):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.fc2(x)\n        return x\n\nclass ClassBlock(nn.Module):\n    def __init__(self, dim,  mlp_ratio, norm_layer=nn.LayerNorm, cm_type = 'mlp'):\n        super().__init__()\n        # self.norm1 = norm_layer(dim)\n        self.norm2 = norm_layer(dim)\n        self.attn = MambaLayer(dim) #MambaBlock(d_model=dim)\n        if cm_type == 'EinFFT':\n            self.mlp = EinFFT(dim)\n        else:\n            self.mlp = FFN(dim, int(dim * mlp_ratio))  \n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n        elif isinstance(m, nn.Conv2d):\n            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n            fan_out //= m.groups\n            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n            if m.bias is not None:\n                m.bias.data.zero_()\n\n    def forward(self, x, H, W):\n        cls_embed = x[:, :1]\n        cls_embed = cls_embed + self.attn(x[:, :1])\n        cls_embed = cls_embed + self.mlp(self.norm2(cls_embed), H, W)\n        return torch.cat([cls_embed, x[:, 1:]], dim=1)\n\n\n\nclass Block_mamba(nn.Module):\n    def __init__(self, \n        dim, \n        mlp_ratio,\n        drop_path=0., \n        norm_layer=nn.LayerNorm, \n        sr_ratio=1, \n        cm_type = 'mlp'\n    ):\n        super().__init__()\n        # self.norm1 = norm_layer(dim)\n        self.norm2 = norm_layer(dim)\n        self.attn = MambaLayer(dim) #MambaBlock(d_model=dim)\n        if cm_type == 'EinFFT':\n            self.mlp = EinFFT(dim)\n        else:\n            self.mlp = PVT2FFN(in_features=dim, hidden_features=int(dim * mlp_ratio))       \n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n        elif isinstance(m, nn.Conv2d):\n            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n            fan_out //= m.groups\n            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n            if m.bias is not None:\n                m.bias.data.zero_()\n\n    def forward(self, x, H, W):\n        x = x + self.drop_path(self.attn(x))\n        x = x + self.drop_path(self.mlp(self.norm2(x), H, W))\n        return x\n\n\n\nclass DownSamples(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.proj = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1)\n        self.norm = nn.LayerNorm(out_channels)\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n        elif isinstance(m, nn.Conv2d):\n            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n            fan_out //= m.groups\n            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n            if m.bias is not None:\n                m.bias.data.zero_()\n\n    def forward(self, x):\n        x = self.proj(x)\n        _, _, H, W = x.shape\n        x = x.flatten(2).transpose(1, 2)\n        x = self.norm(x)\n        return x, H, W\n\nclass Stem(nn.Module):\n    def __init__(self, in_channels, stem_hidden_dim, out_channels):\n        super().__init__()\n        hidden_dim = stem_hidden_dim\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_channels, hidden_dim, kernel_size=7, stride=2,\n                      padding=3, bias=False),  # 112x112\n            nn.BatchNorm2d(hidden_dim),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(hidden_dim, hidden_dim, kernel_size=3, stride=1,\n                      padding=1, bias=False),  # 112x112\n            nn.BatchNorm2d(hidden_dim),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(hidden_dim, hidden_dim, kernel_size=3, stride=1,\n                      padding=1, bias=False),  # 112x112\n            nn.BatchNorm2d(hidden_dim),\n            nn.ReLU(inplace=True),\n        )\n        self.proj = nn.Conv2d(hidden_dim,\n                              out_channels,\n                              kernel_size=3,\n                              stride=2,\n                              padding=1)\n        self.norm = nn.LayerNorm(out_channels)\n\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n        elif isinstance(m, nn.Conv2d):\n            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n            fan_out //= m.groups\n            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n            if m.bias is not None:\n                m.bias.data.zero_()\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.proj(x)\n        _, _, H, W = x.shape\n        x = x.flatten(2).transpose(1, 2)\n        x = self.norm(x)\n        return x, H, W\n\nclass SiMBA(nn.Module):\n    def __init__(self, \n        in_chans=3, \n        num_classes=1000, \n        stem_hidden_dim = 32,\n        embed_dims=[64, 128, 320, 448],\n        mlp_ratios=[8, 8, 4, 4], \n        drop_path_rate=0., \n        norm_layer=nn.LayerNorm,\n        depths=[3, 4, 6, 3], \n        sr_ratios=[4, 2, 1, 1], \n        num_stages=4,\n        token_label=True,\n        cm_type='mlp',\n        **kwargs\n    ):\n        super().__init__()\n        self.num_classes = num_classes\n        self.depths = depths\n        self.num_stages = num_stages\n\n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule\n        cur = 0\n        alpha=5#\n        for i in range(num_stages):\n            if i == 0:\n                patch_embed = Stem(in_chans, stem_hidden_dim, embed_dims[i])\n            else:\n                patch_embed = DownSamples(embed_dims[i - 1], embed_dims[i])\n\n            block = nn.ModuleList([Block_mamba(\n                dim = embed_dims[i], \n                mlp_ratio = mlp_ratios[i], \n                drop_path=dpr[cur + j], \n                norm_layer=norm_layer,\n                sr_ratio = sr_ratios[i],\n                cm_type=cm_type)   # Change this to run EinFFT based Channel Mixer, cm_type='EinFFT'\n            for j in range(depths[i])])\n\n            norm = norm_layer(embed_dims[i])\n            cur += depths[i]\n\n            setattr(self, f\"patch_embed{i + 1}\", patch_embed)\n            setattr(self, f\"block{i + 1}\", block)\n            setattr(self, f\"norm{i + 1}\", norm)\n\n        post_layers = ['ca']\n        self.post_network = nn.ModuleList([\n            ClassBlock(\n                dim = embed_dims[-1], \n                mlp_ratio = mlp_ratios[-1],\n                norm_layer=norm_layer,\n                cm_type=cm_type) # Change this to run EinFFT based Channel Mixer, cm_type='EinFFT'\n            for _ in range(len(post_layers))\n        ])\n\n        # classification head\n        self.head = nn.Linear(embed_dims[-1], num_classes) if num_classes > 0 else nn.Identity()\n        ##################################### token_label #####################################\n        self.return_dense = token_label\n        self.mix_token = token_label\n        self.beta = 1.0\n        self.pooling_scale = 8\n        if self.return_dense:\n            self.aux_head = nn.Linear(\n                embed_dims[-1],\n                num_classes) if num_classes > 0 else nn.Identity()\n        ##################################### token_label #####################################\n\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n        elif isinstance(m, nn.Conv2d):\n            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n            fan_out //= m.groups\n            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n            if m.bias is not None:\n                m.bias.data.zero_()\n\n    def forward_cls(self, x, H, W):\n        B, N, C = x.shape\n        cls_tokens = x.mean(dim=1, keepdim=True)\n        x = torch.cat((cls_tokens, x), dim=1)\n        for block in self.post_network:\n            x = block(x, H, W)\n        return x\n\n    def forward_features(self, x):\n        B = x.shape[0]\n        for i in range(self.num_stages):\n            patch_embed = getattr(self, f\"patch_embed{i + 1}\")\n            block = getattr(self, f\"block{i + 1}\")\n            x, H, W = patch_embed(x)\n            for blk in block:\n                x = blk(x, H, W)\n            \n            if i != self.num_stages - 1:\n                norm = getattr(self, f\"norm{i + 1}\")\n                x = norm(x)\n                x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n\n        x = self.forward_cls(x, H, W)[:, 0]\n        norm = getattr(self, f\"norm{self.num_stages}\")\n        x = norm(x)\n        return x\n\n    def forward(self, x):\n        if not self.return_dense:\n            x = self.forward_features(x)\n            x = self.head(x)\n            return x\n        else:\n            x, H, W = self.forward_embeddings(x)\n            # mix token, see token labeling for details.\n            if self.mix_token and self.training:\n                lam = np.random.beta(self.beta, self.beta)\n                patch_h, patch_w = x.shape[1] // self.pooling_scale, x.shape[\n                    2] // self.pooling_scale\n                bbx1, bby1, bbx2, bby2 = rand_bbox(x.size(), lam, scale=self.pooling_scale)\n                temp_x = x.clone()\n                sbbx1,sbby1,sbbx2,sbby2=self.pooling_scale*bbx1,self.pooling_scale*bby1,\\\n                                        self.pooling_scale*bbx2,self.pooling_scale*bby2\n                temp_x[:, sbbx1:sbbx2, sbby1:sbby2, :] = x.flip(0)[:, sbbx1:sbbx2, sbby1:sbby2, :]\n                x = temp_x\n            else:\n                bbx1, bby1, bbx2, bby2 = 0, 0, 0, 0\n            x = self.forward_tokens(x, H, W)\n            x_cls = self.head(x[:, 0])\n            x_aux = self.aux_head(\n                x[:, 1:]\n            )  # generate classes in all feature tokens, see token labeling\n\n            if not self.training:\n                return x_cls + 0.5 * x_aux.max(1)[0]\n\n            if self.mix_token and self.training:  # reverse \"mix token\", see token labeling for details.\n                x_aux = x_aux.reshape(x_aux.shape[0], patch_h, patch_w, x_aux.shape[-1])\n\n                temp_x = x_aux.clone()\n                temp_x[:, bbx1:bbx2, bby1:bby2, :] = x_aux.flip(0)[:, bbx1:bbx2, bby1:bby2, :]\n                x_aux = temp_x\n\n                x_aux = x_aux.reshape(x_aux.shape[0], patch_h * patch_w, x_aux.shape[-1])\n\n            return x_cls, x_aux, (bbx1, bby1, bbx2, bby2)\n\n    def forward_tokens(self, x, H, W):\n        B = x.shape[0]\n        x = x.view(B, -1, x.size(-1))\n\n        for i in range(self.num_stages):\n            if i != 0:\n                patch_embed = getattr(self, f\"patch_embed{i + 1}\")\n                x, H, W = patch_embed(x)\n            block = getattr(self, f\"block{i + 1}\")\n            for blk in block:\n                x = blk(x, H, W)\n            if i != self.num_stages - 1:\n                norm = getattr(self, f\"norm{i + 1}\")\n                x = norm(x)\n                x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n        x = self.forward_cls(x, H, W)\n        norm = getattr(self, f\"norm{self.num_stages}\")\n        x = norm(x)    \n        return x\n\n    def forward_embeddings(self, x):\n        patch_embed = getattr(self, f\"patch_embed{0 + 1}\")\n        x, H, W = patch_embed(x)\n        x = x.view(x.size(0), H, W, -1)\n        return x, H, W\n\n\nclass DWConv(nn.Module):\n    def __init__(self, dim=768):\n        super(DWConv, self).__init__()\n        self.dwconv = nn.Conv2d(dim, dim, 3, 1, 1, bias=True, groups=dim)\n\n    def forward(self, x, H, W):\n        B, N, C = x.shape\n        x = x.transpose(1, 2).view(B, C, H, W)\n        x = self.dwconv(x)\n        x = x.flatten(2).transpose(1, 2)\n        return x\n\n@register_model\ndef simba_s(pretrained=False, **kwargs):\n    model = SiMBA(\n        stem_hidden_dim = 32,\n        embed_dims = [64, 128, 320, 448], \n        mlp_ratios = [8, 8, 4, 4],\n        norm_layer = partial(nn.LayerNorm, eps=1e-6), \n        depths = [3, 4, 6, 3], \n        sr_ratios = [4, 2, 1, 1], \n        **kwargs)\n    model.default_cfg = _cfg()\n    return model\n\n@register_model\ndef simba_b(pretrained=False, **kwargs):\n    model = SiMBA(\n        stem_hidden_dim = 64,\n        embed_dims = [64, 128, 320, 512], \n        mlp_ratios = [8, 8, 4, 4], \n        norm_layer = partial(nn.LayerNorm, eps=1e-6), \n        depths = [3, 4, 12, 3], \n        sr_ratios = [4, 2, 1, 1], \n        **kwargs)\n    model.default_cfg = _cfg()\n    return model\n\n@register_model\ndef simba_l(pretrained=False, **kwargs):\n    model = SiMBA(\n        stem_hidden_dim = 64,\n        embed_dims = [96, 192, 384, 512],\n        mlp_ratios = [8, 8, 4, 4],\n        norm_layer = partial(nn.LayerNorm, eps=1e-6), \n        depths = [3, 6, 18, 3], \n        sr_ratios = [4, 2, 1, 1], \n        **kwargs)\n    model.default_cfg = _cfg()\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-08T21:07:59.282753Z","iopub.execute_input":"2024-11-08T21:07:59.283302Z","iopub.status.idle":"2024-11-08T21:07:59.406832Z","shell.execute_reply.started":"2024-11-08T21:07:59.283263Z","shell.execute_reply":"2024-11-08T21:07:59.405979Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"simba = SiMBA(\n        stem_hidden_dim = 32,\n        num_classes=10, \n        embed_dims = [64, 128], \n        mlp_ratios = [4, 2],\n        norm_layer = partial(nn.LayerNorm, eps=1e-6), \n        depths = [2, 3],\n        num_stages = 2,\n        sr_ratios = [4, 2],\n        cm_type = 'mlp',\n        token_label = False)\nsimba = nn.DataParallel(simba)\nsimba.to('cuda')\n\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(simba.parameters(), lr=1e-3, eps=0.0003)\n\nprint(count_parameters(simba))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-08T14:32:40.728589Z","iopub.execute_input":"2024-11-08T14:32:40.729643Z","iopub.status.idle":"2024-11-08T14:32:40.792890Z","shell.execute_reply.started":"2024-11-08T14:32:40.729601Z","shell.execute_reply":"2024-11-08T14:32:40.791838Z"}},"outputs":[{"name":"stdout","text":"1178474\n","output_type":"stream"}],"execution_count":58},{"cell_type":"code","source":"num_epochs = 21\nfor epoch in range(num_epochs):  # loop over the dataset multiple times\n    for inputs, labels in trainloader:\n        # zero the parameter gradients\n        optimizer.zero_grad()\n        # print(inputs.dtype)\n        # forward + backward + optimize\n        outputs = simba(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n    val = evaluate(simba, testloader)\n    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Val_acc: {val:.2f}')\n\nprint('Finished Training')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-08T14:16:18.546408Z","iopub.execute_input":"2024-11-08T14:16:18.546766Z","iopub.status.idle":"2024-11-08T14:27:04.110093Z","shell.execute_reply.started":"2024-11-08T14:16:18.546735Z","shell.execute_reply":"2024-11-08T14:27:04.108786Z"}},"outputs":[{"name":"stdout","text":"Epoch [1/21], Loss: 1.4239, Val_acc: 52.56\nEpoch [2/21], Loss: 1.3047, Val_acc: 62.18\nEpoch [3/21], Loss: 1.0611, Val_acc: 67.18\nEpoch [4/21], Loss: 0.9034, Val_acc: 68.87\nEpoch [5/21], Loss: 0.7551, Val_acc: 71.70\nEpoch [6/21], Loss: 0.8602, Val_acc: 71.66\nEpoch [7/21], Loss: 0.3571, Val_acc: 73.08\nEpoch [8/21], Loss: 0.5105, Val_acc: 74.92\nEpoch [9/21], Loss: 0.3820, Val_acc: 73.97\nEpoch [10/21], Loss: 0.4261, Val_acc: 73.82\nEpoch [11/21], Loss: 0.2061, Val_acc: 73.37\nEpoch [12/21], Loss: 0.2671, Val_acc: 73.38\nEpoch [13/21], Loss: 0.2747, Val_acc: 75.66\nEpoch [14/21], Loss: 0.0868, Val_acc: 75.05\nEpoch [15/21], Loss: 0.1246, Val_acc: 74.21\nEpoch [16/21], Loss: 0.0618, Val_acc: 74.08\nEpoch [17/21], Loss: 0.1509, Val_acc: 74.62\nEpoch [18/21], Loss: 0.2445, Val_acc: 73.80\nEpoch [19/21], Loss: 0.1824, Val_acc: 74.03\nEpoch [20/21], Loss: 0.2704, Val_acc: 74.98\nEpoch [21/21], Loss: 0.0336, Val_acc: 74.97\nFinished Training\n","output_type":"stream"}],"execution_count":52},{"cell_type":"code","source":"correct = 0\ntotal = 0\n# since we're not training, we don't need to calculate the gradients for our outputs\nwith torch.no_grad():\n    for data in trainloader:\n        images, labels = data\n        # calculate outputs by running images through the network\n        outputs = simba(images)\n        # the class with the highest energy is what we choose as prediction\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels.to(device)).sum().item()\n\nprint(f'Accuracy of the network on the 10000 test images: {100 * correct / total:.2f} %')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-08T14:27:04.112710Z","iopub.execute_input":"2024-11-08T14:27:04.113594Z","iopub.status.idle":"2024-11-08T14:27:20.784072Z","shell.execute_reply.started":"2024-11-08T14:27:04.113540Z","shell.execute_reply":"2024-11-08T14:27:20.782928Z"}},"outputs":[{"name":"stdout","text":"Accuracy of the network on the 10000 test images: 97.97 %\n","output_type":"stream"}],"execution_count":53},{"cell_type":"code","source":"simba = SiMBA(\n        stem_hidden_dim = 32,\n        num_classes=10, \n        embed_dims = [32, 256], \n        mlp_ratios = [2, 4],\n        norm_layer = partial(nn.LayerNorm, eps=1e-6), \n        depths = [3, 2],\n        num_stages = 2,\n        sr_ratios = [4, 2],\n        cm_type = 'EinFFT',\n        token_label = False)\nsimba = nn.DataParallel(simba)\nsimba.to('cuda')\n\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(simba.parameters(), lr=1e-3, eps=0.0003)\n\nprint(count_parameters(simba))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-08T21:10:01.819463Z","iopub.execute_input":"2024-11-08T21:10:01.820348Z","iopub.status.idle":"2024-11-08T21:10:01.893007Z","shell.execute_reply.started":"2024-11-08T21:10:01.820307Z","shell.execute_reply":"2024-11-08T21:10:01.891935Z"}},"outputs":[{"name":"stdout","text":"1908746\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"num_epochs = 21\nfor epoch in range(num_epochs):  # loop over the dataset multiple times\n    for inputs, labels in trainloader:\n        # zero the parameter gradients\n        optimizer.zero_grad()\n        # print(inputs.dtype)\n        # forward + backward + optimize\n        outputs = simba(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n    val = evaluate(simba, testloader)\n    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Val_acc: {val:.2f}')\n\nprint('Finished Training')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-08T21:10:08.085747Z","iopub.execute_input":"2024-11-08T21:10:08.086776Z","iopub.status.idle":"2024-11-08T21:23:37.989149Z","shell.execute_reply.started":"2024-11-08T21:10:08.086720Z","shell.execute_reply":"2024-11-08T21:23:37.987984Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/tmp/ipykernel_30/3148824784.py:37: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/native/Copy.cpp:305.)\n  x = x.to(torch.float32)\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/21], Loss: 1.2882, Val_acc: 51.75\nEpoch [2/21], Loss: 0.9905, Val_acc: 59.78\nEpoch [3/21], Loss: 0.8882, Val_acc: 65.60\nEpoch [4/21], Loss: 0.8957, Val_acc: 68.15\nEpoch [5/21], Loss: 0.7609, Val_acc: 69.62\nEpoch [6/21], Loss: 0.6948, Val_acc: 69.75\nEpoch [7/21], Loss: 0.4352, Val_acc: 71.32\nEpoch [8/21], Loss: 0.4932, Val_acc: 71.01\nEpoch [9/21], Loss: 0.4304, Val_acc: 70.30\nEpoch [10/21], Loss: 0.3568, Val_acc: 71.08\nEpoch [11/21], Loss: 0.2797, Val_acc: 70.32\nEpoch [12/21], Loss: 0.3022, Val_acc: 70.65\nEpoch [13/21], Loss: 0.1784, Val_acc: 70.76\nEpoch [14/21], Loss: 0.3074, Val_acc: 70.86\nEpoch [15/21], Loss: 0.2911, Val_acc: 70.17\nEpoch [16/21], Loss: 0.1818, Val_acc: 70.92\nEpoch [17/21], Loss: 0.0872, Val_acc: 70.96\nEpoch [18/21], Loss: 0.1443, Val_acc: 70.31\nEpoch [19/21], Loss: 0.0263, Val_acc: 71.55\nEpoch [20/21], Loss: 0.2221, Val_acc: 70.91\nEpoch [21/21], Loss: 0.0784, Val_acc: 71.65\nFinished Training\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"correct = 0\ntotal = 0\n# since we're not training, we don't need to calculate the gradients for our outputs\nwith torch.no_grad():\n    for data in trainloader:\n        images, labels = data\n        # calculate outputs by running images through the network\n        outputs = simba(images)\n        # the class with the highest energy is what we choose as prediction\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels.to(device)).sum().item()\n\nprint(f'Accuracy of the network on the 10000 test images: {100 * correct / total:.2f} %')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-08T21:24:05.907346Z","iopub.execute_input":"2024-11-08T21:24:05.907772Z","iopub.status.idle":"2024-11-08T21:24:27.594933Z","shell.execute_reply.started":"2024-11-08T21:24:05.907730Z","shell.execute_reply":"2024-11-08T21:24:27.593864Z"}},"outputs":[{"name":"stdout","text":"Accuracy of the network on the 10000 test images: 98.11 %\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"# Refined SiMBA","metadata":{}},{"cell_type":"code","source":"import math\nimport torch.nn as nn\nfrom mamba_ssm import Mamba\nfrom einops.layers.torch import Rearrange\nfrom timm.models.layers import DropPath, to_2tuple, trunc_normal_\n\nclass EinFFT(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.hidden_size = dim #768\n        self.num_blocks = 4 \n        self.block_size = self.hidden_size // self.num_blocks \n        assert self.hidden_size % self.num_blocks == 0\n        self.sparsity_threshold = 0.01\n        self.scale = 0.02\n\n        self.complex_weight_1 = nn.Parameter(torch.randn(2, self.num_blocks, self.block_size, self.block_size, dtype=torch.float32) * self.scale)\n        self.complex_weight_2 = nn.Parameter(torch.randn(2, self.num_blocks, self.block_size, self.block_size, dtype=torch.float32) * self.scale)\n        self.complex_bias_1 = nn.Parameter(torch.randn(2, self.num_blocks, self.block_size,  dtype=torch.float32) * self.scale)\n        self.complex_bias_2 = nn.Parameter(torch.randn(2, self.num_blocks, self.block_size,  dtype=torch.float32) * self.scale)\n\n    def multiply(self, input, weights):\n        return torch.einsum('...bd,bdk->...bk', input, weights)\n\n    def forward(self, x, H, W):\n        B, N, C = x.shape\n        x = x.view(B, N, self.num_blocks, self.block_size )\n\n        x = torch.fft.fft2(x, dim=(1,2), norm='ortho') # FFT on N dimension\n\n        x_real_1 = F.relu(self.multiply(x.real, self.complex_weight_1[0]) - self.multiply(x.imag, self.complex_weight_1[1]) + self.complex_bias_1[0])\n        x_imag_1 = F.relu(self.multiply(x.real, self.complex_weight_1[1]) + self.multiply(x.imag, self.complex_weight_1[0]) + self.complex_bias_1[1])\n        x_real_2 = self.multiply(x_real_1, self.complex_weight_2[0]) - self.multiply(x_imag_1, self.complex_weight_2[1]) + self.complex_bias_2[0]\n        x_imag_2 = self.multiply(x_real_1, self.complex_weight_2[1]) + self.multiply(x_imag_1, self.complex_weight_2[0]) + self.complex_bias_2[1]\n\n        x = torch.stack([x_real_2, x_imag_2], dim=-1).float()\n        x = F.softshrink(x, lambd=self.sparsity_threshold) if self.sparsity_threshold else x\n        x = torch.view_as_complex(x)\n\n        x = torch.fft.ifft2(x, dim=(1,2), norm=\"ortho\")\n        \n        # RuntimeError: \"fused_dropout\" not implemented for 'ComplexFloat'\n        x = x.to(torch.float32)\n        x = x.reshape(B, N, C)\n        return x\n\n\nclass PVT2FFN(nn.Module):\n    def __init__(self, in_features, hidden_features):\n        super().__init__()\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.dwconv = DWConv(hidden_features)\n        self.act = nn.GELU()\n        self.fc2 = nn.Linear(hidden_features, in_features)\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n        elif isinstance(m, nn.Conv2d):\n            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n            fan_out //= m.groups\n            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n            if m.bias is not None:\n                m.bias.data.zero_()\n\n    def forward(self, x, H, W):\n        x = self.fc1(x)\n        x = self.dwconv(x, H, W)\n        x = self.act(x)\n        x = self.fc2(x)\n        return x\n\nclass DWConv(nn.Module):\n    def __init__(self, dim=768):\n        super(DWConv, self).__init__()\n        self.dwconv = nn.Conv2d(dim, dim, 3, 1, 1, bias=True, groups=dim)\n\n    def forward(self, x, H, W):\n        #print(x.shape)\n        B, N, C = x.shape\n        x = x.transpose(1, 2).view(B, C, H, W)\n        x = self.dwconv(x)\n        x = x.flatten(2).transpose(1, 2)\n        return x\n\n\n\nclass RSimbaBlock(nn.Module):\n    def __init__(self, embed_dim, cm_type='mlp'):\n        super().__init__()\n        self.mamba = Mamba(d_model=embed_dim, d_state=16, d_conv=8, expand=2)\n        if cm_type == 'mlp':\n            self.mlp = PVT2FFN(in_features=embed_dim, hidden_features=int(embed_dim * 2))\n        else:\n            self.mlp = EinFFT(embed_dim)\n        self.norm = nn.LayerNorm(embed_dim)\n        \n\n    def forward(self, x):\n        #print(x.shape)\n        x = self.mamba(x) + x + self.mlp(x,8,8)\n        return self.norm(x)\n\nclass RSimbaBackBone(nn.Module):\n    def __init__(self, embed_dim, n_layers, seq_len=None, global_pool=True, cm_type='mlp'):\n        super().__init__()\n        self.blocks = nn.Sequential(*[RSimbaBlock(embed_dim) for _ in range(n_layers)])\n        self.global_pool = global_pool #for classification or other supervised learning.\n\n    def forward(self, x):\n        #for input (bs, n, d) it returns either (bs, n, d) or (bs, d) is global_pool\n        out = self.blocks(x) if not self.global_pool else torch.mean(self.blocks(x),1)\n        return out\n\n\nclass RSiMBA(nn.Module):\n    def __init__(self, patch_size=4, img_size=32, n_channels=3, embed_dim=128, n_layers=6, cm_type='mlp'):\n        super().__init__()\n\n        patch_dim = n_channels*patch_size*patch_size\n        self.rearrange = Rearrange('b c (h p1) (w p2) -> b (h w) (c p1 p2)',\n                                   p1=patch_size, p2=patch_size)\n\n        self.func = nn.Sequential(self.rearrange,\n                                  nn.Linear(patch_dim, embed_dim),\n                                  RSimbaBackBone(embed_dim,6),\n                                  nn.Linear(embed_dim, 10))\n\n    def forward(self, x):\n        #print(x.shape)\n        return self.func(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-09T12:39:46.772758Z","iopub.execute_input":"2024-11-09T12:39:46.773170Z","iopub.status.idle":"2024-11-09T12:39:46.808256Z","shell.execute_reply.started":"2024-11-09T12:39:46.773130Z","shell.execute_reply":"2024-11-09T12:39:46.807335Z"}},"outputs":[],"execution_count":53},{"cell_type":"code","source":"simba = RSiMBA()\nsimba = nn.DataParallel(simba)\nsimba.to('cuda')\n\n    \ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(simba.parameters(), lr=1e-3, eps=0.0003)\n\nprint(count_parameters(simba))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-09T12:39:59.251155Z","iopub.execute_input":"2024-11-09T12:39:59.251864Z","iopub.status.idle":"2024-11-09T12:39:59.296441Z","shell.execute_reply.started":"2024-11-09T12:39:59.251823Z","shell.execute_reply":"2024-11-09T12:39:59.295265Z"}},"outputs":[{"name":"stdout","text":"1125002\n","output_type":"stream"}],"execution_count":54},{"cell_type":"code","source":"xxx = Rearrange('b c (h p1) (w p2) -> b (h w) (c p1 p2)',\n                                   p1=4, p2=4)\ntest = torch.rand(128,3,32,32)\nprint(test.shape)\ntest = xxx(test)\nxxx = Rearrange(' b (h w) (c p1 p2)  -> b c (h p1) (w p2)',\n                                   p1=4, p2=4, h=8,w=8, c = 3)\n\nprint(test.shape)\ntest = xxx(test)\nprint(test.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-09T13:00:34.075270Z","iopub.execute_input":"2024-11-09T13:00:34.076080Z","iopub.status.idle":"2024-11-09T13:00:34.087588Z","shell.execute_reply.started":"2024-11-09T13:00:34.076037Z","shell.execute_reply":"2024-11-09T13:00:34.086666Z"}},"outputs":[{"name":"stdout","text":"torch.Size([128, 3, 32, 32])\ntorch.Size([128, 64, 48])\ntorch.Size([128, 3, 32, 32])\n","output_type":"stream"}],"execution_count":62},{"cell_type":"code","source":"num_epochs = 21\nfor epoch in range(num_epochs):  # loop over the dataset multiple times\n    for inputs, labels in trainloader:\n        # zero the parameter gradients\n        optimizer.zero_grad()\n        #print(inputs.shape)\n        # forward + backward + optimize\n        outputs = simba(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n    val = evaluate(simba, testloader)\n    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Val_acc: {val:.2f}')\n\nprint('Finished Training')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-09T12:40:03.932401Z","iopub.execute_input":"2024-11-09T12:40:03.933069Z","iopub.status.idle":"2024-11-09T12:48:41.251256Z","shell.execute_reply.started":"2024-11-09T12:40:03.933028Z","shell.execute_reply":"2024-11-09T12:48:41.250072Z"}},"outputs":[{"name":"stdout","text":"Epoch [1/21], Loss: 1.2045, Val_acc: 53.11\nEpoch [2/21], Loss: 1.1145, Val_acc: 61.73\nEpoch [3/21], Loss: 0.6917, Val_acc: 68.06\nEpoch [4/21], Loss: 0.6500, Val_acc: 71.66\nEpoch [5/21], Loss: 0.7271, Val_acc: 74.23\nEpoch [6/21], Loss: 0.5736, Val_acc: 74.88\nEpoch [7/21], Loss: 0.3483, Val_acc: 75.22\nEpoch [8/21], Loss: 0.4633, Val_acc: 74.03\nEpoch [9/21], Loss: 0.3855, Val_acc: 73.63\nEpoch [10/21], Loss: 0.1300, Val_acc: 75.00\nEpoch [11/21], Loss: 0.1753, Val_acc: 73.84\nEpoch [12/21], Loss: 0.1425, Val_acc: 74.85\nEpoch [13/21], Loss: 0.0887, Val_acc: 74.27\nEpoch [14/21], Loss: 0.1194, Val_acc: 74.68\nEpoch [15/21], Loss: 0.0904, Val_acc: 75.26\nEpoch [16/21], Loss: 0.1376, Val_acc: 75.69\nEpoch [17/21], Loss: 0.0581, Val_acc: 74.50\nEpoch [18/21], Loss: 0.1201, Val_acc: 76.09\nEpoch [19/21], Loss: 0.0202, Val_acc: 75.10\nEpoch [20/21], Loss: 0.2076, Val_acc: 74.93\nEpoch [21/21], Loss: 0.0319, Val_acc: 75.70\nFinished Training\n","output_type":"stream"}],"execution_count":55},{"cell_type":"code","source":"correct = 0\ntotal = 0\n# since we're not training, we don't need to calculate the gradients for our outputs\nwith torch.no_grad():\n    for data in trainloader:\n        images, labels = data\n        # calculate outputs by running images through the network\n        outputs = simba(images)\n        # the class with the highest energy is what we choose as prediction\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels.to(device)).sum().item()\n\nprint(f'Accuracy of the network on the 10000 test images: {100 * correct / total:.2f} %')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-09T12:49:01.652458Z","iopub.execute_input":"2024-11-09T12:49:01.653424Z","iopub.status.idle":"2024-11-09T12:49:15.038964Z","shell.execute_reply.started":"2024-11-09T12:49:01.653379Z","shell.execute_reply":"2024-11-09T12:49:15.037666Z"}},"outputs":[{"name":"stdout","text":"Accuracy of the network on the 10000 test images: 98.36 %\n","output_type":"stream"}],"execution_count":56}]}